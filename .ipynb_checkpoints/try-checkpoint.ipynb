{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc43a1f",
   "metadata": {},
   "source": [
    "## Section 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c40526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextDataset, DataCollatorForLanguageModeling, AutoModelWithLMHead\n",
    "import tqdm as notebook_tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2Model, TrainingArguments, Trainer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66a0326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "album_path = 'data/input/Albums/'\n",
    "\n",
    "album_list = [os.path.join(album_path, a) for a in os.listdir(album_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7079be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "album_dataframes = []\n",
    "\n",
    "for track in album_list:\n",
    "    # Step 3: List all .txt files within the subfolder\n",
    "    tracks = [f.path for f in os.scandir(track) if f.is_file() and f.name.endswith('.txt')]\n",
    "     \n",
    "    # Step 4: Read each .txt file into a Pandas DataFrame and append to the list\n",
    "    for track in tracks:\n",
    "        df = pd.read_csv(track, sep='\\t') #read each line, or each song ... (?)\n",
    "        album_dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "285beed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics = pd.concat([df.iloc[:, 0] for df in album_dataframes], ignore_index=True)\n",
    "final_df = pd.DataFrame({'all_lyrics': all_lyrics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b76ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to filter bad quality data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198eed96",
   "metadata": {},
   "source": [
    "## Section 1 - Choosing a metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20fd31",
   "metadata": {},
   "source": [
    "Choosing a metric... BLEU or ROUGE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f5a5d",
   "metadata": {},
   "source": [
    "## Section 2 - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "863c4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # try nltk tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb2d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(all_lyrics,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5dcecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 29854\n",
      "Test dataset length: 5269\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset length: \"+str(len(train)))\n",
    "print(\"Test dataset length: \"+ str(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a3ac7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/output/trainTaylorSwift.txt', 'w', encoding='utf-8') as f:\n",
    "  for t in train:\n",
    "    f.write(t)\n",
    "    f.write(' ')\n",
    "\n",
    "\n",
    "with open('data/output/testTaylorSwift.txt', 'w', encoding='utf-8') as f:\n",
    "  for t in test:\n",
    "    f.write(t)\n",
    "    f.write(' ')\n",
    "    \n",
    "train_path = 'data/output/trainTaylorSwift.txt'\n",
    "test_path = 'data/output/testTaylorSwift.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "405a210f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acf66f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-taylorswift\", \n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2, \n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps = 100, \n",
    "    save_steps=800, \n",
    "    warmup_steps=500\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d07bb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 57:58, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.102800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.368400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1080, training_loss=3.699252404106988, metrics={'train_runtime': 3484.5114, 'train_samples_per_second': 1.239, 'train_steps_per_second': 0.31, 'total_flos': 282064748544000.0, 'train_loss': 3.699252404106988, 'epoch': 2.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cf695cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x25442f79ab0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab1dab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.to_json_file(\"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05b45c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model ./gpt2-taylorswift with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, <class 'transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 563, in from_pretrained\n    return model_class.from_pretrained(\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\modeling_utils.py\", line 2740, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./gpt2-taylorswift.\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 563, in from_pretrained\n    return model_class.from_pretrained(\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2740, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named tf_model.h5 or pytorch_model.bin found in directory ./gpt2-taylorswift.\n\nwhile loading with GPT2LMHeadModel, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\modeling_utils.py\", line 2740, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./gpt2-taylorswift.\n\nwhile loading with TFGPT2LMHeadModel, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2740, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named tf_model.h5 or pytorch_model.bin found in directory ./gpt2-taylorswift.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m taylor \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./gpt2-taylorswift\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m generator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm a language model,\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\pipelines\\__init__.py:824\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    823\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 824\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    825\u001b[0m         model,\n\u001b[0;32m    826\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    827\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    828\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    829\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    830\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    834\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    835\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\pipelines\\base.py:282\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    281\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    283\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m         )\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model ./gpt2-taylorswift with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, <class 'transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 563, in from_pretrained\n    return model_class.from_pretrained(\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\modeling_utils.py\", line 2740, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./gpt2-taylorswift.\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 563, in from_pretrained\n    return model_class.from_pretrained(\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2740, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named tf_model.h5 or pytorch_model.bin found in directory ./gpt2-taylorswift.\n\nwhile loading with GPT2LMHeadModel, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\modeling_utils.py\", line 2740, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./gpt2-taylorswift.\n\nwhile loading with TFGPT2LMHeadModel, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\anmatos\\AppData\\Local\\miniconda3\\envs\\condaEnv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2740, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named tf_model.h5 or pytorch_model.bin found in directory ./gpt2-taylorswift.\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "taylor = pipeline('text-generation', model='./gpt2-taylorswift', tokenizer=tokenizer)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c001db",
   "metadata": {},
   "source": [
    "## ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3071bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
